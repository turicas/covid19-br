[ðŸ‡ºðŸ‡¸ English?](README.en.md)

# covid19-br

![pytest@docker](https://github.com/turicas/covid19-br/workflows/pytest@docker/badge.svg) ![goodtables](https://github.com/turicas/covid19-br/workflows/goodtables/badge.svg)

Esse repositÃ³rio centraliza links e dados sobre boletins de nÃºmero de casos das
Secretarias Estaduais de SaÃºde (SES) sobre os casos de covid19 no Brasil (por
municÃ­pio por dia), alÃ©m de outros dados relevantes para a anÃ¡lise, como Ã³bitos
registrados em cartÃ³rio (por estado por dia).


## Tabela de ConteÃºdos
1. [LicenÃ§a e CitaÃ§Ã£o](#licena-e-citao)
2. [Sobre os dados](#dados)
3. [Guia de contribuiÃ§Ã£o geral](#contribuindo)
4. [Guia de instalaÃ§Ã£o / setup do projeto (ambiente de desenvolvimento)](#instalando)
5. [Guia de como executar os scrapers existentes](#executando-os-scrapers)
6. [Guia de como criar novos scrapers](#criando-novos-scrapers)
7. [Guia de como atualizar os dados no Brasil.io (ambiente de produÃ§Ã£o)](#atualizao-dos-dados-no-brasilio)

## LicenÃ§a e CitaÃ§Ã£o

A licenÃ§a do cÃ³digo Ã© [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e
dos dados convertidos [Creative Commons Attribution
ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Caso utilize os
dados, **cite a fonte original e quem tratou os dados** e caso compartilhe os
dados, **utilize a mesma licenÃ§a**.
Exemplos de como os dados podem ser citados:
- **Fonte: Secretarias de SaÃºde das Unidades Federativas, dados tratados por Ãlvaro Justen e equipe de voluntÃ¡rios [Brasil.IO](https://brasil.io/)**
- **Brasil.IO: boletins epidemiolÃ³gicos da COVID-19 por municÃ­pio por dia, disponÃ­vel em: https://brasil.io/dataset/covid19/ (Ãºltima atualizaÃ§Ã£o pode ser conferida no site).**


## Dados

Depois de coletados e checados os dados ficam disponÃ­veis de 3 formas no
[Brasil.IO](https://brasil.io/):

- [Interface Web](https://brasil.io/dataset/covid19) (feita para humanos)
- [API](https://brasil.io/api/dataset/covid19) (feita para humanos que desenvolvem programas) - [veja a documentaÃ§Ã£o da API](api.md)
- [Download do dataset completo](https://data.brasil.io/dataset/covid19/_meta/list.html)

Caso queira acessar os dados antes de serem publicados (ATENÃ‡ÃƒO: pode ser que
nÃ£o tenham sido checados), vocÃª pode [acessar diretamente as planilhas em que
estamos trabalhando](https://drive.google.com/open?id=1l3tiwrGEcJEV3gxX0yP-VMRNaE1MLfS2).

Se esse programa e/ou os dados resultantes foram Ãºteis a vocÃª ou Ã  sua empresa,
**considere [fazer uma doaÃ§Ã£o ao projeto Brasil.IO](https://brasil.io/doe)**,
que Ã© mantido voluntariamente.


### FAQ SOBRE OS DADOS

**Antes de entrar em contato conosco (estamos sobrecarregados) para tirar
dÃºvidas sobre os dados, [CONSULTE NOSSO FAQ](faq.md).**

Para mais detalhes [veja a metodologia de coleta de
dados](https://drive.google.com/open?id=1escumcbjS8inzAKvuXOQocMcQ8ZCqbyHU5X5hFrPpn4).


### Clipping

Quer saber quais projetos e notÃ­cias estÃ£o usando nossos dados? [Veja o
clipping](clipping.md).


### Analisando os dados

Caso queira analisar os dados usando SQL, veja o script
[`analysis.sh`](analysis.sh) (ele baixa e converte os CSVs para um banco de
dados SQLite e jÃ¡ cria Ã­ndices e *views* que facilitam o trabalho) e os
arquivos na pasta [`sql/`](sql/).

Por padrÃ£o o script reutiliza os arquivos
caso jÃ¡ tenha baixado; para sempre baixar a versÃ£o mais atual dos dados,
execute `./analysis.sh --clean`.

Leia tambÃ©m nossa [anÃ¡lise dos microdados de vacinaÃ§Ã£o disponÃ­veis no
OpenDataSUS](analises/microdados-vacinacao/README.md).


### Validando os dados

Os metadados estÃ£o descritos conforme os padrÃµes *Data Package* e
*[Table Schema](https://specs.frictionlessdata.io/table-schema/#language)* do
*[Frictionless Data](https://frictionlessdata.io/)*. Isso significa que os
dados podem ser validados automaticamente para detectar, por exemplo, se os
valores de um campo estÃ£o em conformidade com a tipagem definida, se uma data
Ã© vÃ¡lida, se hÃ¡ colunas faltando ou se hÃ¡ linhas duplicadas.

Para fazer a verificaÃ§Ã£o, ative o ambiente virtual Python e em seguida digite:

```
goodtables data/datapackage.json
```

O relatÃ³rio da ferramenta
*[Good Tables](https://github.com/frictionlessdata/goodtables-py)* irÃ¡ indicar
se houver alguma inconsistÃªncia. A validaÃ§Ã£o tambÃ©m pode ser feita *online*
pelo site [Goodtables.io](http://goodtables.io/).


### Mais informaÃ§Ãµes

VocÃª pode ter interesse em ver tambÃ©m:
- [Outros datasets relevantes](datasets-relevantes.md)
- [RecomendaÃ§Ãµes para secretarias de saÃºde na disponibilizaÃ§Ã£o de
  dados](recomendacoes.md)


## Contribuindo

VocÃª pode contribuir de diversas formas:

- Criando programas (crawlers/scrapers/spiders) para extrair os dados automaticamente ([LEIA ESSE GUIA ANTES](#criando-novos-scrapers));
- Coletando links para os boletins de seu estado;
- Coletando dados sobre os casos por municÃ­pio por dia;
- Entrando em contato com a secretaria estadual de seu estado, sugerindo as
  [recomendaÃ§Ãµes de liberaÃ§Ã£o dos dados](recomendacoes.md);
- Evitando contato com humanos;
- Lavando as mÃ£os vÃ¡rias vezes ao dia;
- Sendo solidÃ¡rio aos mais vulnerÃ¡veis;

Para se voluntariar, [siga estes passos](CONTRIBUTING.md).

Procure o seu estado [nas issues desse
repositÃ³rio](https://github.com/turicas/covid19-br/issues) e vamos conversar
por lÃ¡.


## Instalando

Este projeto utiliza Python 3 (testado em 3.8.2) e Scrapy.

VocÃª pode montar seu ambiente de desenvolvimento utilizando o
[setup padrÃ£o](#setup-padro) ou o [setup com docker](#setup-com-docker).

### Setup PadrÃ£o

1. Instale o Python 3.8.2
2. Crie um virtualenv (vocÃª pode usar
  [venv](https://docs.python.org/pt-br/3/library/venv.html) para isso).
3. Instale as dependÃªncias: `pip install -r requirements-development.txt`


### Setup com Docker

Se vocÃª preferir utilizar o Docker para executar, basta usar os comandos a seguir :

```shell
make docker-build       # para construir a imagem
make docker-run-spiders # para coletar os dados
```

## Executando os scrapers

Uma vez que seu [setup](#instalando) estiver terminado, vocÃª pode rodar **todos os
scrapers** usando um dos seguintes comandos no seu terminal (a depender do tipo de
setup que decidiu fazer):

```shell
python covid19br/run_spider.py  # caso tenha feito o setup padrÃ£o
make docker-run-spiders         # caso esteja usando o setup com docker
```

Os comandos acima irÃ£o rodar os scrapers de **todos os estados** que temos implementado
buscando os dados sobre a **data de hoje** e **salvarÃ£o o consolidado** em `.csv` na pasta
`data` deste diretÃ³rio (por padrÃ£o sÃ£o salvos em arquivos com o nome no padrÃ£o 
`"data/{estado}/covid19-{estado}-{data}{extra_info}.csv"`).

Mas essa nÃ£o Ã© a Ãºnica forma de usar esse comando, vocÃª pode optar por nÃ£o salvar os
consolidados em um `.csv` (apenas exibi-los na tela) ou entÃ£o rodar apenas os scrapers
de alguns estados especÃ­ficos ou para outros dias especÃ­ficos que nÃ£o sÃ£o necessariamente
a data de hoje.

Para adaptar melhor o comando ao seu caso de uso vocÃª pode rodÃ¡-lo no terminal com
as seguintes opÃ§Ãµes:

> OBS: Se vocÃª estiver usando docker, basta acrescentar `docker container run --rm
 --name covid19-br -v $(PWD)/data:/app/data covid19-br` antes de qualquer um dos
  comandos a seguir.

```shell
# Exemplo de como raspar os dados de todos os estados em um intervalo de datas
python covid19br/run_spider.py --start_date 24/02/2021 --end_date 30/03/2021

# Caso vocÃª queira executar para datas especÃ­ficas (coloque-as em lista separando-as por vÃ­rgulas):
python covid19br/run_spider.py --dates_range  15/01/2022,17/01/2022

# Para executar apenas spiders de estados especÃ­ficos (coloque-os em lista e separados por vÃ­rgulas):
python covid19br/run_spider.py --states BA,PR

# Para ver quais sÃ£o os estados com scrapers implementados:
python covid19br/run_spider.py --available_spiders

# Caso vocÃª nÃ£o queira salvar os csv's, apenas mostrar na tela os resultados:
python covid19br/run_spider.py --print_results_only

# VocÃª pode consultar essas e outras opÃ§Ãµes disponÃ­veis usando:
python covid19br/run_spider.py -h
```

## Criando novos scrapers

Estamos mudando a forma como subimos os dados para facilitar o trabalho dos
voluntÃ¡rios e deixar o processo mais robusto e confiÃ¡vel e, com isso, serÃ¡
mais fÃ¡cil que robÃ´s possam subir tambÃ©m os dados; dessa forma, os scrapers
ajudarÃ£o *bastante* no processo.

PorÃ©m, ao criar um scraper Ã© importante que vocÃª siga algumas regras:

- **NecessÃ¡rio** fazer o scraper usando o `scrapy` (confira [aqui as docs](https://scrapy.org/));
- **NÃ£o usar** `pandas`, `BeautifulSoap`, `requests` ou outras bibliotecas
  desnecessÃ¡rias (a std lib do Python jÃ¡ tem muita biblioteca Ãºtil, o `scrapy`
  com XPath jÃ¡ dÃ¡ conta de boa parte das raspagens e `rows` jÃ¡ Ã© uma dependÃªncia
  desse repositÃ³rio);

Para padronizar a forma que os scrapers recebem parÃ¢metros e retornam os dados,
criamos um [Spider Base](covid19br/common/base_spider.py), que nada mais Ã© que
um spider bÃ¡sico do scrapy com lÃ³gica a mais para:
- Identificar _para quais datas_ o spider deve procurar dados (essa informaÃ§Ã£o
  Ã© recebida como parÃ¢metro e Ã© guardada na classe no atributo `self.dates_range`,
  que Ã© um `generator` de valores do tipo `datetime.date` com as datas que precisamos
  raspar os dados, e deve ser usada pelo seu spider para buscar os dados como
  solicitado).
- Guardar os dados raspados de uma forma que sejam retornados para o
  sistema que chamou o scraper de forma padronizada.

Para padronizar os dados que sÃ£o retornados pelos spiders, criamos a classe
[FullReport](covid19br/common/models/full_report.py) que representa um "relatÃ³rio
completo" e armazena todos os dados coletados para um determinado estado em uma
data especÃ­fica. Esse relatÃ³rio completo Ã© composto por vÃ¡rios [boletins](covid19br/common/models/bulletin_models.py),
(um para cada cidade do estado + um para casos importados/indefinidos) com o
nÃºmero de casos confirmados e nÃºmero de mortes daquele dia.

O seu script nÃ£o precisa se preocupar com a criaÃ§Ã£o do objeto `FullReport` que serÃ¡
retornado, isso Ã© responsabilidade do [Spider Base](covid19br/common/base_spider.py),
o que seu spider deve criar sÃ£o os [boletins](covid19br/common/models/bulletin_models.py)
com os dados que ele coletar e salvar esses `boletins` no relatÃ³rio atravÃ©s do mÃ©todo
`add_new_bulletin_to_report` disponibilizado pelo `Spider Base`.

Em resumo, ao criar um spider de um novo estado tenha em mente:
- Ã‰ desejÃ¡vel que vocÃª crie seu spider extendendo a classe [Spider Base](covid19br/common/base_spider.py)
  (vocÃª pode conferir alguns exemplos de como outros spiders sÃ£o implementados na pasta
  [/covid19br/spiders](covid19br/spiders)).
- Um spider completo Ã© capaz de coletar os dados:
    - De nÃºmero de casos confirmados e nÃºmero de mortes **por cidade** do estado;
    - De nÃºmero de casos confirmados e nÃºmero de mortes **importados/indefinidos**;
    - De nÃºmeros de casos confirmados e nÃºmeros de mortes **totais** do estado (esse
      valor normalmente Ã© computado automaticamente conforme os casos acimas sÃ£o obtidos,
      mas em casos onde a scretaria disponibiliza o valor total, nÃ³s optamos por usÃ¡-lo
      como "fonte da verdade").
    - Para diferentes datas (desde o inÃ­cio da pandemia atÃ© hoje).
    > OBS: Como nÃ£o hÃ¡ uma padronizaÃ§Ã£o na forma em que as secretarias disponibilizam os
    dados, nem sempre Ã© possÃ­vel obter todas essas informaÃ§Ãµes como desejamos. Obter parte
    dessas informaÃ§Ãµes de forma automatizada jÃ¡ pode ser um bom comeÃ§o e uma contribuiÃ§Ã£o vÃ¡lida! :)
- Os dados coletados devem ser salvos em [boletins](covid19br/common/models/bulletin_models.py)
  e adicionados no retorno do spider atravÃ©s do mÃ©todo `add_new_bulletin_to_report`.

Ao finalizar a implementaÃ§Ã£o do seu spider, adicione-o na lista de spiders do script
[run_spider.py](covid19br/run_spider.py) e execute-o (mais informaÃ§Ãµes sobre como fazer
isso na seÃ§Ã£o anterior). Se tudo correu como previsto, Ã© esperado que seja criado um `.csv`
na pasta `/data/...` com os dados raspados pelo seu spider :)

Nesse momento nÃ£o temos muito tempo disponÃ­vel para revisÃ£o, entÃ£o **por favor**,
sÃ³ crie um *pull request* com cÃ³digo de um novo scraper caso vocÃª possa cumprir os
requisitos acima.


## AtualizaÃ§Ã£o dos Dados no Brasil.IO

Crie um arquivo `.env` com os valores corretos para as seguintes variÃ¡veis de
ambiente:

```shell
BRASILIO_SSH_USER
BRASILIO_SSH_SERVER
BRASILIO_DATA_PATH
BRASILIO_UPDATE_COMMAND
BULLETIN_SPREADSHEET_ID
```

Execute o script:

`./deploy.sh full`

Ele irÃ¡ coletar os dados das planilhas (que estÃ£o linkadas em
`data/boletim_url.csv` e `data/caso_url.csv`), adicionar os dados ao
repositÃ³rio, compactÃ¡-los, enviÃ¡-los ao servidor e executar o comando de
atualizaÃ§Ã£o de dataset.

> Nota: o script que baixa e converte os dados automaticamente deve ser
> executado separadamente, com o comando `./run-spiders.sh`.
