
[üá∫üá∏ English?](README.en.md)

# covid19-br

![pytest@docker](https://github.com/turicas/covid19-br/workflows/pytest@docker/badge.svg)

<!-- TABLE OF CONTENTS -->
## Tabela de Conte√∫do

* [Sobre este Projeto](#sobre-este-projeto)
  * [Feito com](#feito-com)
* [Iniciando](#iniciando)
  * [Pre-requisitos](#pre-requisitos)
  * [Instala√ß√£o](#instala√ß√£o)
* [Utiliza√ß√£o](#utiliza√ß√£o)
* [Roadmap](#roadmap)
* [Contribuindo](#contribuindo)
* [Licen√ßa e Cita√ß√£o](#licen√ßa-e-cita√ß√£o)
* [Contato](#contato)
* [Agradecimentos](#agradecimentos)



<!-- ABOUT THE PROJECT -->
## Sobre este Projeto

Esse reposit√≥rio centraliza links e dados sobre boletins de n√∫mero de casos das
Secretarias Estaduais de Sa√∫de (SES) sobre os casos de covid19 no Brasil (por
munic√≠pio por dia), al√©m de outros dados relevantes para a an√°lise, como √≥bitos
registrados em cart√≥rio (por estado por dia).

### Feito com

* [Python](https://www.python.org/)
* [Bash](https://www.gnu.org/software/bash/)


<!-- GETTING STARTED -->
## Iniciando

TODO

### Pre-requisitos

Instale o Python 3.8.2

### Instala√ß√£o

1. Download, crie &. ative um virtualenv
  - `git clone https://github.com/turicas/covid19-br.git`
  - `virtualenv venv`
  - `source venv/bin/activate`
2. Instale as depend√™ncias:
  - Script de consolida√ß√£o e rob√¥: `pip install -r requirements.txt`
  - Extratores de dados estaduais: `pip install -r requirements-collect.txt`
3. Rode o script de coleta: 
  - `./collect.sh`
  - O objetivo desse script √© # TODO: adicionar o objetivo desse arquivo, ex pq so tem 3 estados
4. Rode o script de consolida√ß√£o: 
  - `./run.sh`
  - O objetivo desse script √© # TODO: adicionar o objetivo desse arquivo...
5. Verifique o resultado em `data/output`.
```
data
‚îÇ   epidemiological-week.csv
‚îÇ   populacao-estimada-2019.csv   
‚îÇ
‚îî‚îÄ‚îÄ‚îÄdownload
    ‚îÇ   ...
|
‚îî‚îÄ‚îÄ‚îÄerror
    ‚îÇ   ...
|
‚îî‚îÄ‚îÄ‚îÄlog
    ‚îÇ   ...
|
‚îî‚îÄ‚îÄ‚îÄoutput
    ‚îÇ   boletim.csv.gz
    ‚îÇ   caso-ce.csv  
    ‚îÇ   caso-pr.csv  
    ‚îÇ   caso-sp.csv  
    ‚îÇ   caso.csv.gz
````

<!-- USAGE EXAMPLES -->
## Utiliza√ß√£o

TODO


<!-- ROADMAP -->
## Roadmap

TODO, o que precisa ser feito, prioridades etc


<!-- CONTRIBUTING -->
## Contribuindo

Voc√™ pode contribuir de diversas formas:

- Criando programas (crawlers/scrapers/spiders) para extrair os dados automaticamente ([LEIA ISSO ANTES](#criando-scrapers));
- Coletando links para os boletins de seu estado;
- Coletando dados sobre os casos por munic√≠pio por dia;
- Entrando em contato com a secretaria estadual de seu estado, sugerindo as
  [recomenda√ß√µes de libera√ß√£o dos dados](recomendacoes.md);
- Evitando contato com humanos, lavando as m√£os v√°rias vezes ao dia, sendo solid√°rio aos mais vulner√°veis;

Para se voluntariar, [siga estes passos](CONTRIBUTING.md).

Procure o seu estado [nas issues desse
reposit√≥rio](https://github.com/turicas/covid19-br/issues) e vamos conversar
por l√°.

## Dados TODO: melhorar descricao & talvez subdividr

Depois de coletados e checados os dados ficam dispon√≠veis de 3 formas no
[Brasil.IO](https://brasil.io/):

- [Interface Web](https://brasil.io/dataset/covid19) (feita para humanos)
- [API](https://brasil.io/api/dataset/covid19) (feita para humanos que desenvolvem programas) - [veja a documenta√ß√£o da API](api.md)
- [Download do dataset completo](https://data.brasil.io/dataset/covid19/_meta/list.html)

Caso queira acessar os dados antes de serem publicados (ATEN√á√ÉO: pode ser que
n√£o tenham sido checados), voc√™ pode [acessar diretamente as planilhas em que
estamos
trabalhando](https://drive.google.com/open?id=1l3tiwrGEcJEV3gxX0yP-VMRNaE1MLfS2).

Se esse programa e/ou os dados resultantes foram √∫teis a voc√™ ou √† sua empresa,
**considere [fazer uma doa√ß√£o ao projeto Brasil.IO](https://brasil.io/doe)**,
que √© mantido voluntariamente.


### FAQ SOBRE OS DADOS

**Antes de entrar em contato conosco (estamos sobrecarregados) para tirar
d√∫vidas sobre os dados, [CONSULTE NOSSO FAQ](faq.md).**

Para mais detalhes [veja a metodologia de coleta de
dados](https://drive.google.com/open?id=1escumcbjS8inzAKvuXOQocMcQ8ZCqbyHU5X5hFrPpn4).

### Analisando os dados

Caso queira analisar os dados usando SQL, veja o script
[`analysis.sh`](analysis.sh) (ele baixa e converte os CSVs para um banco de
dados SQLite e j√° cria √≠ndices e *views* que facilitam o trabalho) e os
arquivos na pasta [`sql/`](sql/). Por padr√£o o script reutiliza os arquivos
caso j√° tenha baixado; para sempre baixar a vers√£o mais atual dos dados,
execute `./analysis.sh --clean`.

### Validando os dados

Os metadados est√£o descritos conforme os padr√µes *Data Package* e
*[Table Schema](https://specs.frictionlessdata.io/table-schema/#language)* do
*[Frictionless Data](https://frictionlessdata.io/)*. Isso significa que os
dados podem ser validados automaticamente para detectar, por exemplo, se os
valores de um campo est√£o em conformidade com a tipagem definida, se uma data
√© v√°lida, se h√° colunas faltando ou se h√° linhas duplicadas.

Para fazer a verifica√ß√£o, ative o ambiente virtual Python e em seguida digite:

```
goodtables data/datapackage.json
```

O relat√≥rio da ferramenta
*[Good Tables](https://github.com/frictionlessdata/goodtables-py)* ir√° indicar
se houver alguma inconsist√™ncia. A valida√ß√£o tamb√©m pode ser feita *online*
pelo site [Goodtables.io](http://goodtables.io/).

### Criando Scrapers

Estamos mudando a forma de subida dos dados para facilitar o trabalho dos volunt√°rios e deixar o processo mais robusto e confi√°vel e, com isso, ser√° mais f√°cil que rob√¥s possam subir tamb√©m os dados; dessa forma, os scrapers ajudar√£o *bastante* no processo. Por√©m, ao criar um scraper √© importante que voc√™ siga algumas regras:

- **Necess√°rio** fazer o scraper usando o `scrapy`;
- **N√£o usar** `pandas`, `BeautifulSoap`, `requests` ou outras bibliotecas desnecess√°rias (a std lib do Python j√° tem muita biblioteca √∫til, o `scrapy` com XPath j√° d√° conta de boa parte das raspagens e `rows` j√° √© uma depend√™ncia desse reposit√≥rio);
- Deve existir alguma maneira f√°cil de fazer o scraper coletar os boletins e casos para uma data espec√≠fica (mas ele deve ser capaz de identificar para quais datas os dados dispon√≠veis e de capturar v√°rias datas tamb√©m);
- O m√©todo de parsing deve devolver (com `yield`) um dicion√°rio com as seguintes chaves:
  - `date`: data no formato `"YYYY-MM-DD"`
  - `state`: sigla do estado, com 2 caracteres mai√∫sculos (deve ser um atributo da classe do spider e usar `self.state`)
  - `city` (nome do munic√≠pio ou em branco, caso seja o valor do estado, deve ser `None`)
  - `place_type`: `"city"` para munic√≠pio e `"state"` para estado
  - `confirmed`: inteiro, n√∫mero de casos confirmados (ou `None`)
  - `deaths`: inteiro, n√∫mero de mortes naquele dia (ou `None`)
  - **ATEN√á√ÉO**: o scraper deve devolver sempre um registro para o estado que *n√£o seja* a soma dos valores por munic√≠pio (esse dado deve ser extra√≠do da linha "total no estado" no boletim) - essa linha vir√° com a coluna `city` com o valor `None` e `place_type` com `"state"` - esse dado apenas deve vir preenchido como sendo a soma dos valores municipais *caso o boletim n√£o tenha os dados totais*;
- Quando poss√≠vel, use testes automatizados.

Nesse momento n√£o temos muito tempo dispon√≠vel para revis√£o, ent√£o **por favor**, s√≥ crie um *pull request* com c√≥digo de um novo scraper caso voc√™ possa cumprir os requisitos acima.

## VEJA TAMB√âM

- [Outros datasets relevantes](datasets-relevantes.md)
- [Recomenda√ß√µes para secretarias de sa√∫de na disponibiliza√ß√£o de
  dados](recomendacoes.md)


## Atualiza√ß√£o dos Dados no Brasil.IO

Crie um arquivo `.env` com os valores corretos para as seguintes vari√°veis de
ambiente:

```shell
BRASILIO_SSH_USER
BRASILIO_SSH_SERVER
BRASILIO_DATA_PATH
BRASILIO_UPDATE_COMMAND
```

Execute o script:

`./deploy.sh`

Ele ir√° coletar os dados das planilhas (que est√£o linkadas em
`data/boletim_url.csv` e `data/caso_url.csv`), adicionar os dados ao
reposit√≥rio, compact√°-los, envi√°-los ao servidor e executar o comando de
atualiza√ß√£o de dataset.

> Nota: o script que baixa e converte os dados automaticamente deve ser
> executado separadamente, com o comando `./collect.sh`.


<!-- LICENSE -->
## Licen√ßa e Cita√ß√£o

A licen√ßa do c√≥digo √© [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e
dos dados convertidos [Creative Commons Attribution
ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Caso utilize os
dados, **cite a fonte original e quem tratou os dados** e caso compartilhe os
dados, **utilize a mesma licen√ßa**.
Exemplos de como os dados podem ser citados:
- **Fonte: Secretarias de Sa√∫de das Unidades Federativas, dados tratados por √Ålvaro Justen e equipe de volunt√°rios [Brasil.IO](https://brasil.io/)**
- **Brasil.IO: boletins epidemiol√≥gicos da COVID-19 por munic√≠pio por dia, dispon√≠vel em: https://brasil.io/dataset/covid19/ (√∫ltima atualiza√ß√£o: XX de XX de XXXX, acesso em XX de XX de XXXX).**



<!-- CONTACT -->
## Contato

TODO


## Clipping

Quer saber quais projetos e not√≠cias est√£o usando nossos dados? [Veja o
clipping](clipping.md).


<!-- ACKNOWLEDGEMENTS -->
## Agradecimentos

TODO
